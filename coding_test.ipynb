{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a9e033",
   "metadata": {},
   "source": [
    "# Engineering Knowledge AI Agent Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c64327",
   "metadata": {},
   "source": [
    "## 1. Perbedaan REST API vs MCP dalam konteks AI\n",
    "\n",
    "### REST API:\n",
    "- Protokol komunikasi standar untuk integrasi sistem\n",
    "- Stateless, setiap request berdiri sendiri\n",
    "- AI model di-deploy sebagai service endpoint\n",
    "- Client kirim request → server proses → return response\n",
    "\n",
    "### MCP (Model Context Protocol):\n",
    "- Protocol standar khusus untuk AI agents berinteraksi dengan tools/resources\n",
    "- **Key advantage**: AI agent manapun bisa langsung pakai MCP server tanpa perlu ubah banyak kode, **cukup adjust di prompt saja**\n",
    "- AI agent otomatis mengenali tools yang tersedia dari MCP server dan bisa gunakan sesuai kebutuhan\n",
    "- Contoh: MCP server expose database access, file system, calculator - AI agent tinggal pilih tool mana yang diperlukan untuk task tertentu\n",
    "- Standardisasi ini bikin development lebih cepat karena sekali bikin MCP server, semua AI agent bisa pakai\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bagaimana REST API & MCP improve AI use case\n",
    "\n",
    "### REST API:\n",
    "- Deploy model sebagai microservice yang scalable\n",
    "- Mudah diintegrasikan ke aplikasi existing\n",
    "- Load balancing untuk handle traffic tinggi\n",
    "- Versioning model lebih mudah\n",
    "\n",
    "### MCP:\n",
    "- AI agent bisa akses database, file system, external APIs secara dynamic\n",
    "- Meningkatkan akurasi dengan real-time context\n",
    "- Agents bisa gunakan multiple tools sekaligus\n",
    "- Reduce hallucination dengan grounding pada data aktual\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Cara memastikan AI agent jawab dengan benar\n",
    "\n",
    "### 1. Prompt Engineering:\n",
    "- Instruksi yang clear dan specific\n",
    "- Berikan examples dan expected format\n",
    "\n",
    "### 2. RAG (Retrieval Augmented Generation):\n",
    "- Ground jawaban pada data/dokumen yang kita tau valid\n",
    "- Mengurangi hallucination karena model tidak mengarang\n",
    "- Source of truth jelas\n",
    "\n",
    "### 3. Human Evaluation & Monitoring:\n",
    "- Output dinilai langsung oleh user\n",
    "- Track metric kepuasan (thumbs up/down, ratings)\n",
    "- Monitor performa model dari feedback real users\n",
    "- Continuous improvement based on metrics\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Docker/Container dalam konteks AI\n",
    "\n",
    "### Use cases:\n",
    "- **Reproducible Environment**: Package semua dependencies (CUDA, Python libs) jadi satu\n",
    "- **Model Serving**: Deploy model dengan isolation yang jelas\n",
    "- **Development Consistency**: Environment sama dari laptop developer ke server production\n",
    "- **GPU Support**: NVIDIA Container Runtime untuk akses GPU\n",
    "- **Versioning**: Bisa run multiple model versions bersamaan\n",
    "- **Resource Limits**: Control CPU/Memory/GPU usage per container\n",
    "- **Portability**: Deploy dimana saja yang support Docker\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Finetune LLM menggunakan Post-Training\n",
    "\n",
    "Berdasarkan pengalaman, saya menggunakan teknik **Post-Training** yang terdiri dari 2 tahap:\n",
    "\n",
    "### Tahap 1: Unsupervised Training (Domain Adaptation)\n",
    "- Training dengan data **domain yang sama** dengan use case target\n",
    "- **Tujuan**: Meningkatkan pengetahuan model tentang domain spesifik\n",
    "- **Format**: Raw text/corpus dari domain tersebut\n",
    "- **Contoh**: Kalau target medical domain, train dengan medical journals, textbooks\n",
    "\n",
    "### Tahap 2: Supervised Training (Task Alignment)\n",
    "- Training dengan data **domain berbeda** tapi **task yang sama**\n",
    "- **Tujuan**: Mengajarkan model memahami task dan mapping input-output\n",
    "- **Format**: Instruction-response pairs atau Q&A\n",
    "- Model jadi lebih paham \"task ini tujuannya apa dan bagaimana cara mappingnya\"\n",
    "\n",
    "### Efisiensi:\n",
    "- Gunakan **LoRA** atau parameter-efficient methods\n",
    "- Tidak perlu update full model, jadi training lebih ringan\n",
    "- Resource requirement lebih kecil tapi hasil tetap bagus\n",
    "\n",
    "**Benefit pendekatan ini**: Model dapat knowledge dari domain spesifik + kemampuan execute task dengan baik dari exposure ke task patterns yang beragam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac3c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7011c",
   "metadata": {},
   "source": [
    "# 1 Parse Small Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/customers-100000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a86ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6f607",
   "metadata": {},
   "source": [
    "# 2 Parse Large Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db1ad7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6add30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_large_files(filepath:str, chunk_size:int):\n",
    "    chunks = pd.read_csv(filepath, chunksize=chunk_size)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af96cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CHUNK TIME : 0.004730939865112305\n",
      "USING NON CHUNK : 12.08965516090393\n"
     ]
    }
   ],
   "source": [
    "#difference between using chunk size and not\n",
    "filepath = 'data/customers-2000000.csv'\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "chunks = read_large_files(filepath, chunk_size=10000)\n",
    "duration = time.time()-start_time\n",
    "print(f'USING CHUNK TIME : {duration}')\n",
    "\n",
    "start_time = time.time()\n",
    "df = pd.read_csv(filepath)\n",
    "duration = time.time()-start_time\n",
    "print(f'USING NON CHUNK : {duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Index      Customer Id First Name  Last Name  \\\n",
      "0         1  4962fdbE6Bfee6D        Pam     Sparks   \n",
      "1         2  9b12Ae76fdBc9bE       Gina      Rocha   \n",
      "2         3  39edFd2F60C85BC    Kristie      Greer   \n",
      "3         4  Fa42AE6a9aD39cE     Arthur     Fields   \n",
      "4         5  F5702Edae925F1D   Michelle    Blevins   \n",
      "...     ...              ...        ...        ...   \n",
      "9995   9996  0eA8b60A83fDB0B     Marvin     Deleon   \n",
      "9996   9997  0B426BAc82F4de7     Carlos   Jennings   \n",
      "9997   9998  eAFCeD87EE37Dd3    Maureen        May   \n",
      "9998   9999  bC8D48359ba1e57      Sandy     Horton   \n",
      "9999  10000  AA8464EbF1Dd2FA       Kyle  Blanchard   \n",
      "\n",
      "                         Company               City  \\\n",
      "0                   Patel-Deleon         Blakemouth   \n",
      "1        Acosta, Paul and Barber   East Lynnchester   \n",
      "2                      Ochoa PLC        West Pamela   \n",
      "3                     Moyer-Wang       East Belinda   \n",
      "4                  Shah and Sons         West Jared   \n",
      "...                          ...                ...   \n",
      "9995   Benton, Weber and Farrell         East Ellen   \n",
      "9996    Mann, Patton and Blevins      Brittneyshire   \n",
      "9997    Black, Friedman and Peck  East Cassidyshire   \n",
      "9998                Lester Group       South Travis   \n",
      "9999  Bullock, Branch and Jensen         East Caleb   \n",
      "\n",
      "                                                Country  \\\n",
      "0     British Indian Ocean Territory (Chagos Archipe...   \n",
      "1                                            Costa Rica   \n",
      "2                                               Ecuador   \n",
      "3                                           Afghanistan   \n",
      "4                                      Marshall Islands   \n",
      "...                                                 ...   \n",
      "9995                                            Belarus   \n",
      "9996                                           Paraguay   \n",
      "9997                                              Congo   \n",
      "9998                                             Poland   \n",
      "9999                                             Panama   \n",
      "\n",
      "                     Phone 1                Phone 2  \\\n",
      "0           267-243-9490x035       480-078-0535x889   \n",
      "1               027.142.0940  +1-752-593-4777x07171   \n",
      "2       +1-049-168-7497x5053        +1-311-216-7855   \n",
      "3     001-653-754-7486x65787       521-630-3858x953   \n",
      "4                 8735278329      (633)283-6034x500   \n",
      "...                      ...                    ...   \n",
      "9995     (736)061-8646x86288     354.292.5659x91130   \n",
      "9996    001-766-455-7179x811     653.112.6383x65155   \n",
      "9997       746-842-2984x6456        +1-483-207-6137   \n",
      "9998     (285)973-5950x31052           211.220.3568   \n",
      "9999        001-129-005-2347       001-883-782-5209   \n",
      "\n",
      "                              Email Subscription Date  \\\n",
      "0     nicolas00@faulkner-kramer.com        2020-11-29   \n",
      "1                yfarley@morgan.com        2021-01-03   \n",
      "2             jennyhayden@petty.org        2021-06-20   \n",
      "3             igrimes@ruiz-todd.org        2020-02-13   \n",
      "4          diamondcarter@jordan.com        2020-10-20   \n",
      "...                             ...               ...   \n",
      "9995   rayperry@douglas-maxwell.com        2021-11-13   \n",
      "9996             bclayton@stein.biz        2021-09-28   \n",
      "9997            daniel19@santos.com        2021-01-21   \n",
      "9998         tonidavenport@diaz.com        2020-03-01   \n",
      "9999           hliu@moyer-patel.com        2020-01-04   \n",
      "\n",
      "                          Website  \n",
      "0             https://nelson.com/  \n",
      "1      https://pineda-rogers.biz/  \n",
      "2           https://mckinney.com/  \n",
      "3          https://dominguez.biz/  \n",
      "4        http://murillo-ryan.com/  \n",
      "...                           ...  \n",
      "9995          https://dawson.com/  \n",
      "9996       http://www.grimes.com/  \n",
      "9997      http://www.parrish.org/  \n",
      "9998    https://www.matthews.com/  \n",
      "9999  http://boyer-henderson.com/  \n",
      "\n",
      "[10000 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#just to see one chunk\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e1ac7",
   "metadata": {},
   "source": [
    "# 3 Difference Between Reading Large and Small Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678eed1",
   "metadata": {},
   "source": [
    "so the difference between reading large file and small file is using chunk when reading with pandas. This make the load process not all at once but per-chunk and we will retrieve each chunk later based on needs. We can also do it without pandas like this [link](https://stackoverflow.com/questions/17444679/reading-a-huge-csv-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a0dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_large_file(filepath):\n",
    "    with open(filepath, mode ='r')as file:\n",
    "        csv_file = csv.reader(file)\n",
    "        for row in csv_file:\n",
    "            # do something with row\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ad6e97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Index', 'Customer Id', 'First Name', 'Last Name', 'Company', 'City', 'Country', 'Phone 1', 'Phone 2', 'Email', 'Subscription Date', 'Website']\n",
      "['1', '4962fdbE6Bfee6D', 'Pam', 'Sparks', 'Patel-Deleon', 'Blakemouth', 'British Indian Ocean Territory (Chagos Archipelago)', '267-243-9490x035', '480-078-0535x889', 'nicolas00@faulkner-kramer.com', '2020-11-29', 'https://nelson.com/']\n",
      "['2', '9b12Ae76fdBc9bE', 'Gina', 'Rocha', 'Acosta, Paul and Barber', 'East Lynnchester', 'Costa Rica', '027.142.0940', '+1-752-593-4777x07171', 'yfarley@morgan.com', '2021-01-03', 'https://pineda-rogers.biz/']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for row in parse_large_file(filepath):\n",
    "    i+=1\n",
    "    print(row)\n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d10667",
   "metadata": {},
   "source": [
    "so using csv reader we parse the csv one by one using yield generator. Using this yield generator wont use any memory so it's good to process large file one by one per-row and not processing all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b59641",
   "metadata": {},
   "source": [
    "# 4 Make Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575ea92",
   "metadata": {},
   "source": [
    "Assumption is this vector db will be used for UI Platform in next question. So for this i will have vector db like this\n",
    "\n",
    "| description    | quantity | price | date |\n",
    "| -------------- | -------- | ----- | ---- |\n",
    "| burger         | 1        | 1000  | 05-11-2025 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9160703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing_extensions import List\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class Receipt(BaseModel):\n",
    "    description : str\n",
    "    quantity : int\n",
    "    price : float\n",
    "    date : str\n",
    "    vendor : str\n",
    "\n",
    "class Receipts(BaseModel):\n",
    "    receipts : List[Receipt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89d7239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d55e1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing_extensions import List, Dict\n",
    "import json\n",
    "\n",
    "from src.be.constant import VECTOR_DB_JSON, VECTOR_DB_EMBEDDING_DIM, VECTOR_DB_ADD_BATCH_SIZE\n",
    "\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, embedding_dimension:int=VECTOR_DB_EMBEDDING_DIM, add_batch_size:int=VECTOR_DB_ADD_BATCH_SIZE):\n",
    "        try:\n",
    "            with open(VECTOR_DB_JSON, 'r') as f:\n",
    "                self.table = json.load(f)\n",
    "        except:\n",
    "            print('Initializing Empty Table')\n",
    "            self.table = []\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            dimensions=embedding_dimension\n",
    "        )\n",
    "        self.add_batch_size = add_batch_size\n",
    "\n",
    "    def add_item(self, receipts:List[Dict]):\n",
    "        for i in range(0, len(receipts), self.add_batch_size):\n",
    "            batch_receipt = receipts[i:i+self.add_batch_size]\n",
    "            batch_description = [receipt['description'] for receipt in batch_receipt]\n",
    "            batch_embeddings = self.make_embedding(batch_description)\n",
    "            self.table.extend([{**batch_receipt[batch_idx], 'embeddings':batch_embeddings[batch_idx]} for batch_idx in range(len(batch_receipt))])\n",
    "        with open(VECTOR_DB_JSON, 'w') as fp:\n",
    "            json.dump(self.table, fp)\n",
    "\n",
    "    def get_item(self):\n",
    "        return [{k:v for k,v in data.items() if k!='embeddings'} for data in self.table]\n",
    "\n",
    "    def make_embedding(self, texts:List[str]):\n",
    "        return self.embeddings.embed_documents(texts) \n",
    "    \n",
    "    def dot_product(self, l1, l2):\n",
    "        assert len(l1)==len(l2), f\"Array size not same, len l1 {len(l1)} len l2 {len(l2)}\"\n",
    "        dot_result = 0\n",
    "        for i in range(len(l1)):\n",
    "            dot_result+=(l1[i]*l2[i])\n",
    "        return dot_result\n",
    "    \n",
    "    def magnitude(self, l):\n",
    "        sum_l = 0\n",
    "        for i in range(len(l)):\n",
    "            sum_l+=l[i]**2\n",
    "        return sum_l**0.5\n",
    "    \n",
    "    def calculate_distance(self, embedding_1:List, embedding_2:List):\n",
    "        return self.dot_product(embedding_1, embedding_2)/(self.magnitude(embedding_1)*self.magnitude(embedding_2))\n",
    "    \n",
    "    def search(self, query:str, top_n:int):\n",
    "        query_embedding = self.make_embedding([query])[0]\n",
    "        top_n_results = []\n",
    "        for i,receipt in enumerate(self.table):\n",
    "            distance = self.calculate_distance(query_embedding, receipt['embeddings'])\n",
    "            if len(top_n_results)>top_n:\n",
    "                if distance>top_n_results[-1]['distance']:\n",
    "                    top_n_results[-1] = {'index':i, 'distance':distance}\n",
    "                    top_n_results = sorted(top_n_results, key=lambda x:x['distance'], reverse=True)\n",
    "            else:\n",
    "                top_n_results.append({'index':i, 'distance':distance})\n",
    "                top_n_results = sorted(top_n_results, key=lambda x:x['distance'], reverse=True)\n",
    "        print(top_n_results)\n",
    "        top_n_results = [{k:v for k,v in self.table[top_n['index']].items() if k!='embeddings'} for top_n in top_n_results]\n",
    "        return top_n_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = Receipt(description='burger', quantity=1, price=60000.0, date='2025-11-04', vendor='bmb')\n",
    "r2 = Receipt(description='nasi goreng', quantity=1, price=12000.0, date='2025-11-03', vendor='nasi goreng mafia')\n",
    "r3 = Receipt(description='mie ayam', quantity=1, price=17000.0, date='2025-11-02', vendor='bakmie gm')\n",
    "r4 = Receipt(description='bakso', quantity=1, price=15000.0, date='2025-11-01', vendor='bakso lapangan tembak senayan')\n",
    "r5 = Receipt(description='sate', quantity=1, price=18000.0, date='2025-11-01', vendor='sate hj budi')\n",
    "r6 = Receipt(description='iga bakar', quantity=1, price=150000.0, date='2025-11-02', vendor='daeng tata')\n",
    "r7 = Receipt(description='kentang goreng', quantity=1, price=20000.0, date='2025-10-30', vendor='mcd')\n",
    "r8 = Receipt(description='es krim', quantity=1, price=8000.0, date='2025-10-27', vendor='mcd')\n",
    "r9 = Receipt(description='jus', quantity=1, price=15000.0, date='2025-10-15', vendor='jus kode')\n",
    "r10 = Receipt(description='es teh', quantity=1, price=5000.0, date='2025-10-29', vendor='solaria')\n",
    "receipts = Receipts(receipts=[\n",
    "    r1,r2,r3,r4,r5,r6,r7,r8,r9,r10\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c32ad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Empty Table\n"
     ]
    }
   ],
   "source": [
    "vector_db = VectorDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d0111d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.add_item(receipts.model_dump()['receipts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "377b93db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': 0, 'distance': 0.9999999999999998}, {'index': 3, 'distance': 0.4241921288922651}, {'index': 1, 'distance': 0.4214685026955221}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'description': 'burger',\n",
       "  'quantity': 1,\n",
       "  'price': 60000.0,\n",
       "  'date': '2025-11-04',\n",
       "  'vendor': 'bmb'},\n",
       " {'description': 'bakso',\n",
       "  'quantity': 1,\n",
       "  'price': 15000.0,\n",
       "  'date': '2025-11-01',\n",
       "  'vendor': 'bakso lapangan tembak senayan'},\n",
       " {'description': 'nasi goreng',\n",
       "  'quantity': 1,\n",
       "  'price': 12000.0,\n",
       "  'date': '2025-11-03',\n",
       "  'vendor': 'nasi goreng mafia'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.search('burger', 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopee_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
